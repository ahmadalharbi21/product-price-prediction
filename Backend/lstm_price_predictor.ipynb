{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9434369,"sourceType":"datasetVersion","datasetId":5732177},{"sourceId":9434714,"sourceType":"datasetVersion","datasetId":5732449}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_excel(\"/kaggle/input/ddssssd/Online Retail.xlsx\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-25T10:48:55.440444Z","iopub.execute_input":"2024-09-25T10:48:55.441108Z","iopub.status.idle":"2024-09-25T10:50:25.565463Z","shell.execute_reply.started":"2024-09-25T10:48:55.441069Z","shell.execute_reply":"2024-09-25T10:50:25.564535Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# List of descriptions to drop\ndescriptions_to_drop = [\n    'Manual', 'Adjust bad debt', 'AMAZON FEE', 'DOTCOM POSTAGE', \n    'Bank Charges', 'POSTAGE', 'SAMPLES', 'Discount', \n    'CRUK Commission', 'CARRIAGE'\n]\n# Dropping rows where 'Description' is in descriptions_to_drop\ndf = df[~df['Description'].isin(descriptions_to_drop)]","metadata":{"execution":{"iopub.status.busy":"2024-09-25T10:50:25.567043Z","iopub.execute_input":"2024-09-25T10:50:25.567579Z","iopub.status.idle":"2024-09-25T10:50:25.806741Z","shell.execute_reply.started":"2024-09-25T10:50:25.567542Z","shell.execute_reply":"2024-09-25T10:50:25.805689Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = df[['Description', 'Country', 'UnitPrice']]\nprint(type(df))","metadata":{"execution":{"iopub.status.busy":"2024-09-25T10:50:25.807855Z","iopub.execute_input":"2024-09-25T10:50:25.808185Z","iopub.status.idle":"2024-09-25T10:50:25.844925Z","shell.execute_reply.started":"2024-09-25T10:50:25.808152Z","shell.execute_reply":"2024-09-25T10:50:25.843994Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n# List of descriptions to drop\ndescriptions_to_drop = [\n    'Manual', 'Adjust bad debt', 'AMAZON FEE', 'DOTCOM POSTAGE',\n    'Bank Charges', 'POSTAGE', 'SAMPLES', 'Discount',\n    'CRUK Commission', 'CARRIAGE'\n]\n\n# Dropping rows where 'Description' is in descriptions_to_drop\ndf = df[~df['Description'].isin(descriptions_to_drop)]\n\n# Keep only necessary columns\ndf = df[['Description', 'Country', 'UnitPrice']]\n\n# Dropping duplicates\ndf = df.drop_duplicates()\n\n# Dropping rows with missing values\ndf = df.dropna()\n\n# Ensure all descriptions are strings\ndf['Description'] = df['Description'].astype(str)\n\n# Convert descriptions to lowercase\ndf['Description'] = df['Description'].str.lower()\n\n# Drop rows where 'UnitPrice' is less than or equal to 0\ndf = df[df['UnitPrice'] > 0]\n\n# Remove outliers in 'UnitPrice' (e.g., prices above 220)\ndf = df[df['UnitPrice'] < 220]","metadata":{"execution":{"iopub.status.busy":"2024-09-25T11:25:10.949505Z","iopub.execute_input":"2024-09-25T11:25:10.950331Z","iopub.status.idle":"2024-09-25T11:25:11.001419Z","shell.execute_reply.started":"2024-09-25T11:25:10.950281Z","shell.execute_reply":"2024-09-25T11:25:11.000459Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nimport re\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras.optimizers import Adam\n\n# Assuming 'df' is already loaded with your data\n\n# Data preprocessing steps\n# ... [Your existing preprocessing code]\n\n# Clean the descriptions\ndef clean_text(text):    \n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = text.lower()\n    text = ' '.join(text.split())\n    return text\n\ndf['Description'] = df['Description'].apply(clean_text)\n\n\n# Tokenize the descriptions\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(df['Description'])\nX_description_seq = tokenizer.texts_to_sequences(df['Description'])\n\n# Pad the sequences\nmax_sequence_length = 12\nX_description_padded = pad_sequences(X_description_seq, maxlen=max_sequence_length)\n\n# Create an embedding matrix\nvocab_size = min(10000, len(tokenizer.word_index) + 1)\nembedding_dim = 50  # Using GloVe 50d embeddings you already have\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i < vocab_size:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n# Encode 'Country'\nohe_country = OneHotEncoder(drop='first', sparse=False)\nX_country_encoded = ohe_country.fit_transform(df[['Country']])\n\n# Combine all features\nX_combined = np.hstack((X_description_padded, X_country_encoded, additional_features))\n\n# Split data into features (X) and target (y)\ny = df['UnitPrice']\nX_train_combined, X_test_combined, y_train, y_test = train_test_split(\n    X_combined, y, test_size=0.2, random_state=42\n)\n\n# Separate the inputs\nX_train_text = X_train_combined[:, :max_sequence_length]\nX_train_country = X_train_combined[:, max_sequence_length:max_sequence_length + X_country_encoded.shape[1]]\nX_train_additional = X_train_combined[:, -additional_features.shape[1]:]\n\nX_test_text = X_test_combined[:, :max_sequence_length]\nX_test_country = X_test_combined[:, max_sequence_length:max_sequence_length + X_country_encoded.shape[1]]\nX_test_additional = X_test_combined[:, -additional_features.shape[1]:]\n\n# Log transform the target variable\nepsilon = 1e-6\ny_train_log = np.log(y_train + epsilon)\ny_test_log = np.log(y_test + epsilon)\n\n# Build the model with additional features\ntext_input = Input(shape=(max_sequence_length,), name='text_input')\nembedding_layer = Embedding(input_dim=vocab_size,\n                            output_dim=embedding_dim,\n                            weights=[embedding_matrix],\n                            trainable=True)(text_input)\nlstm_layer = LSTM(128)(embedding_layer)\n\ncountry_input = Input(shape=(X_country_encoded.shape[1],), name='country_input')\nadditional_input = Input(shape=(additional_features.shape[1],), name='additional_input')\n\ncombined = concatenate([lstm_layer, country_input, additional_input])\n\ndense1 = Dense(256, activation='relu')(combined)\ndropout1 = Dropout(0.3)(dense1)\ndense2 = Dense(128, activation='relu')(dropout1)\ndropout2 = Dropout(0.3)(dense2)\ndense3 = Dense(64, activation='relu')(dropout2)\n\noutput = Dense(1, name='output')(dense3)\n\nmodel = Model(inputs=[text_input, country_input, additional_input], outputs=output)\n\n# Compile the model\nlearning_rate = 0.0001\noptimizer = Adam(learning_rate=learning_rate)\nmodel.compile(optimizer=optimizer, loss='mean_squared_error')\n\n# Implement early stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    [X_train_text, X_train_country, X_train_additional],\n    y_train_log,\n    epochs=25,\n    batch_size=32,\n    validation_data=([X_test_text, X_test_country, X_test_additional], y_test_log),\n    callbacks=[early_stopping]\n)\n\n# Predict and evaluate\ny_pred_log = model.predict([X_test_text, X_test_country, X_test_additional])\ny_pred = np.exp(y_pred_log.flatten()) - epsilon\ny_pred = np.maximum(y_pred, 0)\n\nmae = mean_absolute_error(y_test, y_pred)\nprint(f'Mean Absolute Error: {mae}')\n\n# Save the model and tokenizer\nmodel.save('unitprice_prediction_model_v4.h5')\nwith open('tokenizer_v4.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\nwith open('ohe_country_v4.pkl', 'wb') as f:\n    pickle.dump(ohe_country, f)\nwith open('scaler_v4.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\nparams = {\n    'max_sequence_length': max_sequence_length,\n    'epsilon': epsilon,\n    'vocab_size': vocab_size,\n    'embedding_dim': embedding_dim\n}\nwith open('model_params_v4.pkl', 'wb') as f:\n    pickle.dump(params, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T11:28:56.154895Z","iopub.execute_input":"2024-09-25T11:28:56.155305Z","iopub.status.idle":"2024-09-25T11:30:54.276647Z","shell.execute_reply.started":"2024-09-25T11:28:56.155266Z","shell.execute_reply":"2024-09-25T11:30:54.275715Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 1.0800 - val_loss: 0.6998\nEpoch 2/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.7008 - val_loss: 0.5180\nEpoch 3/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.5380 - val_loss: 0.4253\nEpoch 4/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.4421 - val_loss: 0.3582\nEpoch 5/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.3769 - val_loss: 0.3370\nEpoch 6/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.3399 - val_loss: 0.3289\nEpoch 7/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.3100 - val_loss: 0.2854\nEpoch 8/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2906 - val_loss: 0.2680\nEpoch 9/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.2758 - val_loss: 0.2876\nEpoch 10/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2569 - val_loss: 0.2625\nEpoch 11/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2433 - val_loss: 0.2512\nEpoch 12/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2435 - val_loss: 0.2427\nEpoch 13/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2334 - val_loss: 0.2484\nEpoch 14/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2268 - val_loss: 0.2415\nEpoch 15/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2182 - val_loss: 0.2407\nEpoch 16/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2185 - val_loss: 0.2453\nEpoch 17/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2039 - val_loss: 0.2610\nEpoch 18/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2121 - val_loss: 0.2323\nEpoch 19/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2005 - val_loss: 0.2301\nEpoch 20/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.2026 - val_loss: 0.2414\nEpoch 21/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1971 - val_loss: 0.2347\nEpoch 22/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1952 - val_loss: 0.2326\nEpoch 23/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1908 - val_loss: 0.2236\nEpoch 24/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1915 - val_loss: 0.2382\nEpoch 25/25\n\u001b[1m813/813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1880 - val_loss: 0.2296\n\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\nMean Absolute Error: 1.1606906261059013\n","output_type":"stream"}]}]}